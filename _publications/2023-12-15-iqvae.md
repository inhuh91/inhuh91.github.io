---
title: "Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning"
collection: publications
category: conferences
permalink: /publication/iqvae
excerpt: 'In Huh, Changwook Jeong, Jae Myung Choe, Young-Gu Kim, Dae Sin Kim'
date: 2023-12-15
venue: 'Neural Information Processing Systems (NeurIPS)'
paperurl: 'https://proceedings.neurips.cc/paper_files/paper/2023/file/7af8e3dfefe6e3141144197b8fa44f79-Paper-Conference.pdf'
# citation: 'Your Name, You. (2024). &quot;Paper Title Number 3.&quot; <i>GitHub Journal of Bugs</i>. 1(3).'
header:
  teaser: iqvae.png
---

We study structure-preserving low-dimensional representation of a data manifold embedded in a high-dimensional observation space based on variational auto-encoders (VAEs). We approach this by decomposing the data manifold $\mathcal{M}$ as $\mathcal{M} = \mathcal{M} / G \times G$, where $G$ and $\mathcal{M} / G$ are a group of symmetry transformations and a quotient space of $\mathcal{M}$ up to $G$, respectively. From this perspective, we define the structure-preserving representation of such a manifold as a latent space $\mathcal{Z}$ which is isometrically isomorphic (i.e., distance-preserving) to the quotient space $\mathcal{M} / G$ rather $\mathcal{M}$ (i.e., symmetry-preserving). To this end, we propose a novel auto-encoding framework, named \textit{isometric quotient VAEs (IQVAEs)}, that can extract the quotient space from observations and learn the Riemannian isometry of the extracted quotient in an unsupervised manner. Empirical proof-of-concept experiments reveal that the proposed method can find a meaningful representation of the learned data and outperform other competitors for downstream tasks.